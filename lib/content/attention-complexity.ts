import { Article } from './types';

export const attentionComplexity: Article = {
  module: 2,
  slug: 'attention-complexity',
  title: 'Attention Mechanics & Complexity',
  description: 'Understanding the computational cost and mechanics of attention operations',
  readTime: 6,
  previousTopic: { module: 2, slug: 'residual-connections', title: '8. Residual Connections' },
  nextTopic: { module: 2, slug: 'causal-masking', title: '10. Causal Masking' },
  content: `# Attention Mechanics & Complexity

## What is Computational Complexity?

Computational complexity tells us **how much work** a process takes as the input grows larger.

Think of it like:
- Organizing 10 books on a shelf? Easy, takes 1 minute
- Organizing 100 books? Takes 10 minutes
- Organizing 1000 books? Takes...?

The **complexity** tells us how the time grows!

## The Attention Problem

Self-attention needs to compare **every word** with **every other word**.

\`\`\`
Sentence: "The cat sat"
Comparisons needed:
- "The" looks at: The, cat, sat (3 comparisons)
- "cat" looks at: The, cat, sat (3 comparisons)
- "sat" looks at: The, cat, sat (3 comparisons)

Total: 3 Ã— 3 = 9 comparisons
\`\`\`

For **n words**: n Ã— n = nÂ² comparisons!

## O(nÂ²) Complexity

This is written as **O(nÂ²)** - "Big-O of n squared"

\`\`\`
Sequence Length (n) â†’ Comparisons (nÂ²)
10 words â†’ 100 comparisons
100 words â†’ 10,000 comparisons
1,000 words â†’ 1,000,000 comparisons! ğŸ’¥
10,000 words â†’ 100,000,000 comparisons!! ğŸ’¥ğŸ’¥
\`\`\`

Notice how it **explodes** as length increases!

## Visual Comparison Matrix

\`\`\`mermaid
graph TD
    subgraph "Attention Matrix (n Ã— n)"
        A["Every word (row)"]
        B["attends to"]
        C["Every word (column)"]
    end
    
    Matrix["Matrix Size: n Ã— n<br/>Total operations: nÂ²"]
    
    A --> B --> C
    B --> Matrix
    
    style Matrix fill:#f59e0b,color:#fff
\`\`\`

## Why is it O(nÂ²)?

### Three Matrix Operations

Attention computes three things for every token pair:

**Step 1: Calculate Scores**
\`\`\`
Query (n Ã— d) Ã— Key^T (d Ã— n) = Scores (n Ã— n)

For each of n queries:
  Compare with n keys
  = n Ã— n = O(nÂ²) operations
\`\`\`

**Step 2: Apply Softmax**
\`\`\`
For each row in the n Ã— n matrix:
  Compute softmax (normalize)
  = O(nÂ²) operations
\`\`\`

**Step 3: Weighted Sum**
\`\`\`
Scores (n Ã— n) Ã— Values (n Ã— d) = Output (n Ã— d)

For each of n positions:
  Sum over n weighted values
  = n Ã— n = O(nÂ²) operations
\`\`\`

**Total: O(nÂ²) + O(nÂ²) + O(nÂ²) = O(nÂ²)**

## Memory Complexity

Not just computation - **memory** also grows!

\`\`\`
Attention Matrix Storage:
- Size: n Ã— n
- For n = 1000: 1,000,000 values
- For n = 10,000: 100,000,000 values

Each value is typically 16 bits (half precision)
= 200 MB for 10,000 tokens! (just for one attention matrix!)
\`\`\`

## The Bottleneck Visualized

\`\`\`mermaid
flowchart LR
    subgraph Input["Input Processing"]
        E1["Embeddings<br/>O(n)"]
    end
    
    subgraph Attention["Attention Layer"]
        A1["Q, K, V projection<br/>O(n)"]
        A2["Attention Matrix<br/>O(nÂ²) âš ï¸"]
        A3["Context computation<br/>O(nÂ²) âš ï¸"]
    end
    
    subgraph FFN["Feed-Forward"]
        F1["FFN computation<br/>O(n)"]
    end
    
    Input --> Attention
    A1 --> A2 --> A3
    Attention --> FFN
    
    style A2 fill:#ef4444,color:#fff
    style A3 fill:#ef4444,color:#fff
\`\`\`

The **attention computation** is the bottleneck!

## Real-World Impact

### Short Sequences (n = 512)

\`\`\`
nÂ² = 512Â² = 262,144 operations
âœ… Fast on modern GPUs
âœ… Fits in memory easily
\`\`\`

### Medium Sequences (n = 2048)

\`\`\`
nÂ² = 2048Â² = 4,194,304 operations
âš ï¸ Slower, but manageable
âš ï¸ More memory needed
\`\`\`

### Long Sequences (n = 100,000)

\`\`\`
nÂ² = 100,000Â² = 10,000,000,000 operations
âŒ Extremely slow
âŒ Requires huge amounts of memory
âŒ Often impossible on standard hardware
\`\`\`

## Comparison with Other Operations

| Operation | Complexity | Example (n=1000) |
|-----------|-----------|------------------|
| **Feed-Forward** | O(n) | 1,000 ops |
| **Layer Norm** | O(n) | 1,000 ops |
| **Embedding Lookup** | O(n) | 1,000 ops |
| **Attention** | O(nÂ²) | 1,000,000 ops âš ï¸ |

Attention is **1000x more expensive** for n=1000!

## The Quadratic Wall

\`\`\`mermaid
graph LR
    subgraph Growth["Complexity Growth"]
        L["Linear O(n)<br/>Grows slowly"]
        Q["Quadratic O(nÂ²)<br/>Grows rapidly"]
    end
    
    Examples["Examples:<br/>n=1000: 1000 vs 1,000,000<br/>n=10000: 10,000 vs 100,000,000"]
    
    L -.Manageable.-> Examples
    Q -.Limiting Factor!.-> Examples
    
    style Q fill:#ef4444,color:#fff
    style L fill:#22c55e,color:#fff
\`\`\`

## Breaking Down the Attention Formula

\`\`\`
Attention(Q, K, V) = Softmax(QK^T / âˆšd) V

Let's count operations for each part:
\`\`\`

### Part 1: QK^T (Query-Key Dot Product)

\`\`\`
Q: [n Ã— d]  (n queries, d dimensions each)
K^T: [d Ã— n]  (n keys transposed)

QK^T: [n Ã— n]

For each of n queries:
  Compute dot product with n keys
  Each dot product: d multiplications
  Total: n Ã— n Ã— d operations
  
Complexity: O(nÂ² Ã— d)
\`\`\`

### Part 2: Divide by âˆšd (Scaling)

\`\`\`
For each element in [n Ã— n] matrix:
  Divide by âˆšd
  
Total: nÂ² operations
Complexity: O(nÂ²)
\`\`\`

### Part 3: Softmax

\`\`\`
For each row (n rows total):
  Exponentiate n values: O(n)
  Sum n values: O(n)
  Divide each by sum: O(n)
  
Total: n Ã— 3n = 3nÂ²
Complexity: O(nÂ²)
\`\`\`

### Part 4: Multiply by V (Apply Attention)

\`\`\`
Attention Weights: [n Ã— n]
V: [n Ã— d]

Result: [n Ã— d]

For each of n output positions:
  Compute weighted sum of n values
  Each value has d dimensions
  Total: n Ã— n Ã— d
  
Complexity: O(nÂ² Ã— d)
\`\`\`

**Dominant Terms: O(nÂ² Ã— d)**

## Multi-Head Attention Complexity

With **h heads**:

\`\`\`
Each head: O(nÂ² Ã— d_head)
where d_head = d_model / h

Total for h heads:
h Ã— O(nÂ² Ã— d/h) = O(nÂ² Ã— d)

Multi-head doesn't change overall complexity!
But processes happen in parallel on GPU âœ…
\`\`\`

## Why Not Just Make Attention Linear?

You might ask: "Why not use a simpler, O(n) operation?"

**The Problem:**
- Attention's power comes from comparing **all pairs**
- This is what enables "understanding" relationships
- Removing this = losing the core benefit

**Trade-off:**
- Power vs Efficiency
- Understanding vs Speed

That's why researchers work on:
- **Efficient attention** (approximations)
- **Sparse attention** (selective comparisons)
- **Linear attention** (new architectures)

We'll cover these later!

## Context Window Limitations

The O(nÂ²) complexity directly limits **context windows**:

| Model | Context Window | Attention Ops |
|-------|---------------|---------------|
| **GPT-2** | 1,024 | ~1 million |
| **GPT-3** | 2,048 | ~4 million |
| **GPT-4** | 8,192 | ~67 million |
| **Claude 2** | 100,000 | ~10 billion âš ï¸ |

Longer context = exponentially more computation!

## The Cost Breakdown

For a transformer with **L layers**, **h heads**, and **n tokens**:

\`\`\`
Per Layer:
- Multi-head attention: O(nÂ² Ã— d)
- Feed-forward: O(n Ã— d Ã— d_ff)

Total Model:
- Attention: L Ã— O(nÂ² Ã— d)
- FFN: L Ã— O(n Ã— d Ã— d_ff)

For n = 2048, d = 768:
- Attention: ~3 billion ops per layer
- FFN: ~6 billion ops per layer

Attention is significant but not always dominant!
\`\`\`

## Practical Implications

**Training:**
- Batch size Ã— sequence length Ã— sequence length
- Limited by GPU memory
- Often use gradient checkpointing to save memory

**Inference:**
- Process one token at a time with KV cache
- Much more efficient (we'll cover this later!)
- Still limited by context length

## Optimization Techniques (Preview)

We'll explore these in upcoming topics:

1. **KV Cache**: Reuse previous computations
2. **Flash Attention**: Optimized GPU kernels
3. **Sparse Attention**: Only attend to subset
4. **Linear Attention**: Approximate with O(n)
5. **PagedAttention**: Better memory management

## Summary

> **Attention Complexity** = O(nÂ²) - every token must compare with every other token, making long sequences computationally expensive.

**Key Insights:**
- ğŸ“Š Quadratic growth: 2x length = 4x computation
- ğŸ’¾ Memory and compute both scale with nÂ²
- ğŸ¯ This is the main bottleneck for long contexts
- âš¡ Many optimizations exist to address this

**The Formula:**
\`\`\`
For n tokens:
Comparisons = nÂ²
Memory = nÂ² Ã— bytes_per_value
Time = O(nÂ²Ã— d)

Where d is the model dimension
\`\`\`

## What's Next?

Now that we understand the computational cost, let's see how transformers handle **sequential generation**.

Next: **Causal Masking** - Preventing the model from "cheating" by looking at future tokens!
`,
};
